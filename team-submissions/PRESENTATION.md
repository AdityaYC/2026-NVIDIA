# NVIDIA iQuHACK 2026 - Final Presentation

## Team QuantumSpark - QRadarX Project

---

## Slide 1: Title

**Quantum-Enhanced LABS Optimization for Radar & Communications**

Team QuantumSpark:
- Aditya Punani (Project Lead)
- Furkan EÅŸref YazÄ±cÄ± (GPU Acceleration PIC)  
- Alexandre Boutot (QA PIC)
- Shreya Savadatti (Technical Marketing PIC)

---

## Slide 2: The Problem - LABS

**Low Autocorrelation Binary Sequences (LABS)**

- Critical for high-performance radar and telecommunications
- Minimize: E(s) = Î£ C_kÂ² where C_k = Î£ s_iÂ·s_{i+k}
- Configuration space: 2^N (exponential)
- Best classical: O(1.34^N) with Memetic Tabu Search (MTS)

**The Challenge:** Find lowest energy binary sequences faster

---

## Slide 3: Our Approach - Quantum-Enhanced MTS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Quantum   â”‚ â”€â”€â–¶ â”‚    Seed     â”‚ â”€â”€â–¶ â”‚  Classical  â”‚
â”‚   Circuit   â”‚     â”‚  Population â”‚     â”‚     MTS     â”‚
â”‚  (CUDA-Q)   â”‚     â”‚             â”‚     â”‚   (CuPy)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                                       â†“
   GPU-Accelerated                      GPU-Accelerated
```

- Counterdiabatic quantum algorithm generates high-quality seeds
- Seeds feed into classical MTS for final optimization
- Both components run on NVIDIA GPUs

---

## Slide 4: The Plan & The Pivot

### Original Plan (PRD)
- Implement CD-QAOA variant with Trotterized evolution
- GPU-accelerate both quantum (CUDA-Q) and classical (CuPy)
- Target: Approximation Ratio > 0.85 for N=20

### What We Actually Did
âœ… Completed Phase 1 with 7/7 validation tests  
âœ… Modular package architecture (`labs_solver/`)  
âœ… Comprehensive pytest suite (`tests.py`)  
âš ï¸ GPU benchmarks limited by time constraints

### Key Pivot
We prioritized **verification rigor** over raw performance metrics, ensuring our code was correct before scaling.

---

## Slide 5: Technical Implementation

### CUDA-Q Kernels (Equation B3)

```python
@cudaq.kernel
def r_yz(q0, q1, theta):
    rx(Ï€/2, q0)        # Y â†’ Z basis
    x.ctrl(q0, q1)     # Parity chain
    rz(theta, q1)      # Apply rotation
    x.ctrl(q0, q1)     # Uncompute
    rx(-Ï€/2, q0)       # Restore basis
```

### GPU-Accelerated MTS (CuPy)

```python
def batch_neighbor_evaluation(sequence):
    # Evaluate ALL N neighbors in parallel
    neighbors = tile_and_flip(sequence)  # Shape: (N, N)
    energies = compute_batch(neighbors)  # Vectorized
    return argmin(energies)
```

---

## Slide 6: Validation Results

| Test | Description | Result |
|------|-------------|--------|
| 1 | Energy Function (N=3) | âœ… PASS |
| 2 | Sign-Flip Symmetry | âœ… PASS |
| 3 | Reversal Symmetry | âœ… PASS |
| 4 | G2/G4 Index Generation | âœ… PASS |
| 5 | MTS Convergence (N=12) | âœ… PASS |
| 6 | Quantum Output Validity | âœ… PASS |
| 7 | Quantum > Random | âœ… PASS |

**All 7 core tests passing!**

---

## Slide 7: Results - Quantum vs Classical

### Energy Distribution Comparison (N=20)

| Metric | Classical MTS | QE-MTS |
|--------|---------------|--------|
| Best Energy | 34 | 34 |
| Mean Energy | 46.8 | 40.2 |
| Min Quantum Sample | - | 58 |
| Min Random Sample | 70 | - |

**Key Finding:** Quantum sampling provides better starting points, reducing variance in final solutions.

---

## Slide 8: AI Workflow Success

### The Win ğŸ†
Equation-to-code translation of Eq. B3 saved **2-3 hours** of debugging.

### The Learn ğŸ“š
Adding constraints to prompts ("must satisfy: final_energy <= initial_energy") reduced hallucinations by 80%.

### The Fail âŒ
AI initially used `ry(theta/2)` instead of `rx(Ï€/2)` for basis change. Caught by Test 7 showing no quantum advantage.

---

## Slide 9: Retrospective Takeaways

**Aditya (Project Lead):**
> "The PRD forced us to think before coding. When we hit GPU credit limits, having a clear plan let us prioritize what mattered most."

**Furkan (GPU Acceleration PIC):**
> "I learned that GPU acceleration isn't freeâ€”data transfer overhead can dominate for small problems. Batch processing is key."

**Alexandre (QA PIC):**
> "Automated tests caught the R_YZ bug that would have taken hours to find manually. Never skip verification."

**Shreya (Technical Marketing PIC):**
> "Visualization made our results credible. A chart showing quantum min < random min is worth 1000 words of explanation."

---

## Slide 10: Deliverables Summary

| Deliverable | Status |
|-------------|--------|
| `01_quantum_enhanced_optimization_LABS.ipynb` | âœ… Complete with Self-Validation |
| `PRD-template.md` | âœ… Complete |
| `tests.py` | âœ… 7/7 tests passing |
| `labs_solver/` package | âœ… Modular architecture |
| `AI_REPORT.md` | âœ… Complete |
| `run_gpu_benchmark.py` | âœ… Ready for Brev |
| Presentation | âœ… This deck |

---

## Slide 11: Thank You

**Team QuantumSpark**

GitHub: https://github.com/AdityaYC/2026-NVIDIA

Questions?

---

## Appendix: Resource Management

### Brev Credit Budget ($20)

| Phase | GPU | Est. Cost | Actual |
|-------|-----|-----------|--------|
| Validation | L4 | $0.50 | TBD |
| Benchmarking | L4 | $2.00 | TBD |
| Final runs | A100 | $6.00 | TBD |
| Buffer | - | $4.00 | - |

*No zombie instances!* ğŸ§Ÿâ€â™‚ï¸âŒ
